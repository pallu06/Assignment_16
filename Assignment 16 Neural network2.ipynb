{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db5dfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ced9700",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34a9832b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read file\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "data =pd.read_csv (\"gas_turbines.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "790ab299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'TEY', 'CDP', 'CO', 'NOX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles=list(data.columns)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cdbee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles[7],titles[10]=titles[10],titles[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc5132a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'NOX', 'CDP', 'CO', 'TEY']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f36419",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b21032f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>NOX</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>TEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>82.722</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>114.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>82.776</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>82.468</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>114.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>82.670</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>82.311</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>79.559</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>111.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>79.917</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>111.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>90.912</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>110.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>93.227</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>110.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>92.498</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>111.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     NOX     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  82.722  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  82.776  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  82.468  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  82.670  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  82.311  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  79.559  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  79.917  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  90.912  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  93.227  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  92.498  10.583   \n",
       "\n",
       "           CO     TEY  \n",
       "0      3.1547  114.70  \n",
       "1      3.2363  114.72  \n",
       "2      3.2012  114.71  \n",
       "3      3.1923  114.72  \n",
       "4      3.2484  114.72  \n",
       "...       ...     ...  \n",
       "15034  4.5186  111.61  \n",
       "15035  4.8470  111.78  \n",
       "15036  7.9632  110.19  \n",
       "15037  6.2494  110.74  \n",
       "15038  4.9816  111.58  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c311cf5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>NOX</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>TEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AT</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.412953</td>\n",
       "      <td>-0.549432</td>\n",
       "      <td>-0.099333</td>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.600006</td>\n",
       "      <td>-0.100705</td>\n",
       "      <td>-0.088588</td>\n",
       "      <td>-0.207495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AP</th>\n",
       "      <td>-0.412953</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.146939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AH</th>\n",
       "      <td>-0.549432</td>\n",
       "      <td>0.042573</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.110272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AFDP</th>\n",
       "      <td>-0.099333</td>\n",
       "      <td>0.040318</td>\n",
       "      <td>-0.119249</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>0.717995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GTEP</th>\n",
       "      <td>-0.049103</td>\n",
       "      <td>0.078575</td>\n",
       "      <td>-0.202784</td>\n",
       "      <td>0.744251</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>0.977042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIT</th>\n",
       "      <td>0.093067</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.247781</td>\n",
       "      <td>0.627254</td>\n",
       "      <td>0.874526</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>-0.231636</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>0.891587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAT</th>\n",
       "      <td>0.338569</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.010859</td>\n",
       "      <td>-0.571541</td>\n",
       "      <td>-0.756884</td>\n",
       "      <td>-0.357320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>-0.720356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.600006</td>\n",
       "      <td>0.256744</td>\n",
       "      <td>0.143061</td>\n",
       "      <td>-0.037299</td>\n",
       "      <td>-0.208496</td>\n",
       "      <td>-0.231636</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.169103</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>-0.102631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CDP</th>\n",
       "      <td>-0.100705</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.182010</td>\n",
       "      <td>0.727152</td>\n",
       "      <td>0.993784</td>\n",
       "      <td>0.887238</td>\n",
       "      <td>-0.744740</td>\n",
       "      <td>-0.169103</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>0.988473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CO</th>\n",
       "      <td>-0.088588</td>\n",
       "      <td>0.041614</td>\n",
       "      <td>0.165505</td>\n",
       "      <td>-0.334207</td>\n",
       "      <td>-0.508259</td>\n",
       "      <td>-0.688272</td>\n",
       "      <td>0.063404</td>\n",
       "      <td>0.316743</td>\n",
       "      <td>-0.520783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.541751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TEY</th>\n",
       "      <td>-0.207495</td>\n",
       "      <td>0.146939</td>\n",
       "      <td>-0.110272</td>\n",
       "      <td>0.717995</td>\n",
       "      <td>0.977042</td>\n",
       "      <td>0.891587</td>\n",
       "      <td>-0.720356</td>\n",
       "      <td>-0.102631</td>\n",
       "      <td>0.988473</td>\n",
       "      <td>-0.541751</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            AT        AP        AH      AFDP      GTEP       TIT       TAT  \\\n",
       "AT    1.000000 -0.412953 -0.549432 -0.099333 -0.049103  0.093067  0.338569   \n",
       "AP   -0.412953  1.000000  0.042573  0.040318  0.078575  0.029650 -0.223479   \n",
       "AH   -0.549432  0.042573  1.000000 -0.119249 -0.202784 -0.247781  0.010859   \n",
       "AFDP -0.099333  0.040318 -0.119249  1.000000  0.744251  0.627254 -0.571541   \n",
       "GTEP -0.049103  0.078575 -0.202784  0.744251  1.000000  0.874526 -0.756884   \n",
       "TIT   0.093067  0.029650 -0.247781  0.627254  0.874526  1.000000 -0.357320   \n",
       "TAT   0.338569 -0.223479  0.010859 -0.571541 -0.756884 -0.357320  1.000000   \n",
       "NOX  -0.600006  0.256744  0.143061 -0.037299 -0.208496 -0.231636  0.009888   \n",
       "CDP  -0.100705  0.131198 -0.182010  0.727152  0.993784  0.887238 -0.744740   \n",
       "CO   -0.088588  0.041614  0.165505 -0.334207 -0.508259 -0.688272  0.063404   \n",
       "TEY  -0.207495  0.146939 -0.110272  0.717995  0.977042  0.891587 -0.720356   \n",
       "\n",
       "           NOX       CDP        CO       TEY  \n",
       "AT   -0.600006 -0.100705 -0.088588 -0.207495  \n",
       "AP    0.256744  0.131198  0.041614  0.146939  \n",
       "AH    0.143061 -0.182010  0.165505 -0.110272  \n",
       "AFDP -0.037299  0.727152 -0.334207  0.717995  \n",
       "GTEP -0.208496  0.993784 -0.508259  0.977042  \n",
       "TIT  -0.231636  0.887238 -0.688272  0.891587  \n",
       "TAT   0.009888 -0.744740  0.063404 -0.720356  \n",
       "NOX   1.000000 -0.169103  0.316743 -0.102631  \n",
       "CDP  -0.169103  1.000000 -0.520783  0.988473  \n",
       "CO    0.316743 -0.520783  1.000000 -0.541751  \n",
       "TEY  -0.102631  0.988473 -0.541751  1.000000  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b92c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization of data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8825ce31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.439778</td>\n",
       "      <td>-0.826644</td>\n",
       "      <td>1.281436</td>\n",
       "      <td>-0.921232</td>\n",
       "      <td>-1.379101</td>\n",
       "      <td>-1.488376</td>\n",
       "      <td>0.585240</td>\n",
       "      <td>1.387845</td>\n",
       "      <td>-1.357331</td>\n",
       "      <td>0.532012</td>\n",
       "      <td>-1.231172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.449601</td>\n",
       "      <td>-0.748647</td>\n",
       "      <td>1.304564</td>\n",
       "      <td>-0.921495</td>\n",
       "      <td>-1.363528</td>\n",
       "      <td>-1.482325</td>\n",
       "      <td>0.585240</td>\n",
       "      <td>1.393002</td>\n",
       "      <td>-1.363676</td>\n",
       "      <td>0.568733</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.434721</td>\n",
       "      <td>-0.686250</td>\n",
       "      <td>1.219086</td>\n",
       "      <td>-0.944385</td>\n",
       "      <td>-1.351309</td>\n",
       "      <td>-1.476275</td>\n",
       "      <td>0.568715</td>\n",
       "      <td>1.363586</td>\n",
       "      <td>-1.360957</td>\n",
       "      <td>0.552938</td>\n",
       "      <td>-1.230541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.413702</td>\n",
       "      <td>-0.623853</td>\n",
       "      <td>1.169060</td>\n",
       "      <td>-0.946884</td>\n",
       "      <td>-1.348194</td>\n",
       "      <td>-1.464173</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>1.382878</td>\n",
       "      <td>-1.356424</td>\n",
       "      <td>0.548933</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.368693</td>\n",
       "      <td>-0.545857</td>\n",
       "      <td>1.161883</td>\n",
       "      <td>-0.924389</td>\n",
       "      <td>-1.354663</td>\n",
       "      <td>-1.458123</td>\n",
       "      <td>0.582698</td>\n",
       "      <td>1.348591</td>\n",
       "      <td>-1.350985</td>\n",
       "      <td>0.574179</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>-1.153182</td>\n",
       "      <td>-1.185428</td>\n",
       "      <td>1.401860</td>\n",
       "      <td>-0.865850</td>\n",
       "      <td>-1.498657</td>\n",
       "      <td>-2.063184</td>\n",
       "      <td>0.103453</td>\n",
       "      <td>1.085751</td>\n",
       "      <td>-1.543161</td>\n",
       "      <td>1.145792</td>\n",
       "      <td>-1.426381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>-1.303986</td>\n",
       "      <td>-1.138630</td>\n",
       "      <td>1.447753</td>\n",
       "      <td>-0.913470</td>\n",
       "      <td>-1.438759</td>\n",
       "      <td>-2.268905</td>\n",
       "      <td>-0.276638</td>\n",
       "      <td>1.119943</td>\n",
       "      <td>-1.513247</td>\n",
       "      <td>1.293578</td>\n",
       "      <td>-1.415642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>-1.386267</td>\n",
       "      <td>-1.076233</td>\n",
       "      <td>1.476971</td>\n",
       "      <td>-0.951488</td>\n",
       "      <td>-1.410967</td>\n",
       "      <td>-2.789257</td>\n",
       "      <td>-1.026650</td>\n",
       "      <td>2.170062</td>\n",
       "      <td>-1.467922</td>\n",
       "      <td>2.695925</td>\n",
       "      <td>-1.516089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>-1.420423</td>\n",
       "      <td>-0.998236</td>\n",
       "      <td>1.441590</td>\n",
       "      <td>-0.988848</td>\n",
       "      <td>-1.447624</td>\n",
       "      <td>-2.456474</td>\n",
       "      <td>-0.528337</td>\n",
       "      <td>2.391165</td>\n",
       "      <td>-1.422598</td>\n",
       "      <td>1.924683</td>\n",
       "      <td>-1.481343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>-1.430734</td>\n",
       "      <td>-0.935839</td>\n",
       "      <td>1.334652</td>\n",
       "      <td>-1.016605</td>\n",
       "      <td>-1.464635</td>\n",
       "      <td>-2.051083</td>\n",
       "      <td>0.057689</td>\n",
       "      <td>2.321539</td>\n",
       "      <td>-1.377273</td>\n",
       "      <td>1.354150</td>\n",
       "      <td>-1.428277</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -1.439778 -0.826644  1.281436 -0.921232 -1.379101 -1.488376  0.585240   \n",
       "1     -1.449601 -0.748647  1.304564 -0.921495 -1.363528 -1.482325  0.585240   \n",
       "2     -1.434721 -0.686250  1.219086 -0.944385 -1.351309 -1.476275  0.568715   \n",
       "3     -1.413702 -0.623853  1.169060 -0.946884 -1.348194 -1.464173  0.583969   \n",
       "4     -1.368693 -0.545857  1.161883 -0.924389 -1.354663 -1.458123  0.582698   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "15034 -1.153182 -1.185428  1.401860 -0.865850 -1.498657 -2.063184  0.103453   \n",
       "15035 -1.303986 -1.138630  1.447753 -0.913470 -1.438759 -2.268905 -0.276638   \n",
       "15036 -1.386267 -1.076233  1.476971 -0.951488 -1.410967 -2.789257 -1.026650   \n",
       "15037 -1.420423 -0.998236  1.441590 -0.988848 -1.447624 -2.456474 -0.528337   \n",
       "15038 -1.430734 -0.935839  1.334652 -1.016605 -1.464635 -2.051083  0.057689   \n",
       "\n",
       "             7         8         9         10  \n",
       "0      1.387845 -1.357331  0.532012 -1.231172  \n",
       "1      1.393002 -1.363676  0.568733 -1.229909  \n",
       "2      1.363586 -1.360957  0.552938 -1.230541  \n",
       "3      1.382878 -1.356424  0.548933 -1.229909  \n",
       "4      1.348591 -1.350985  0.574179 -1.229909  \n",
       "...         ...       ...       ...       ...  \n",
       "15034  1.085751 -1.543161  1.145792 -1.426381  \n",
       "15035  1.119943 -1.513247  1.293578 -1.415642  \n",
       "15036  2.170062 -1.467922  2.695925 -1.516089  \n",
       "15037  2.391165 -1.422598  1.924683 -1.481343  \n",
       "15038  2.321539 -1.377273  1.354150 -1.428277  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=preprocessing.StandardScaler()\n",
    "data1=scaler.fit_transform(data)\n",
    "data2=pd.DataFrame(data1)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b26ef8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data2.iloc[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb27854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.439778</td>\n",
       "      <td>-0.826644</td>\n",
       "      <td>1.281436</td>\n",
       "      <td>-0.921232</td>\n",
       "      <td>-1.379101</td>\n",
       "      <td>-1.488376</td>\n",
       "      <td>0.585240</td>\n",
       "      <td>1.387845</td>\n",
       "      <td>-1.357331</td>\n",
       "      <td>0.532012</td>\n",
       "      <td>-1.231172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.449601</td>\n",
       "      <td>-0.748647</td>\n",
       "      <td>1.304564</td>\n",
       "      <td>-0.921495</td>\n",
       "      <td>-1.363528</td>\n",
       "      <td>-1.482325</td>\n",
       "      <td>0.585240</td>\n",
       "      <td>1.393002</td>\n",
       "      <td>-1.363676</td>\n",
       "      <td>0.568733</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.434721</td>\n",
       "      <td>-0.686250</td>\n",
       "      <td>1.219086</td>\n",
       "      <td>-0.944385</td>\n",
       "      <td>-1.351309</td>\n",
       "      <td>-1.476275</td>\n",
       "      <td>0.568715</td>\n",
       "      <td>1.363586</td>\n",
       "      <td>-1.360957</td>\n",
       "      <td>0.552938</td>\n",
       "      <td>-1.230541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.413702</td>\n",
       "      <td>-0.623853</td>\n",
       "      <td>1.169060</td>\n",
       "      <td>-0.946884</td>\n",
       "      <td>-1.348194</td>\n",
       "      <td>-1.464173</td>\n",
       "      <td>0.583969</td>\n",
       "      <td>1.382878</td>\n",
       "      <td>-1.356424</td>\n",
       "      <td>0.548933</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.368693</td>\n",
       "      <td>-0.545857</td>\n",
       "      <td>1.161883</td>\n",
       "      <td>-0.924389</td>\n",
       "      <td>-1.354663</td>\n",
       "      <td>-1.458123</td>\n",
       "      <td>0.582698</td>\n",
       "      <td>1.348591</td>\n",
       "      <td>-1.350985</td>\n",
       "      <td>0.574179</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.328821</td>\n",
       "      <td>-0.389864</td>\n",
       "      <td>0.984836</td>\n",
       "      <td>-0.890186</td>\n",
       "      <td>-1.374309</td>\n",
       "      <td>-1.452072</td>\n",
       "      <td>0.581427</td>\n",
       "      <td>1.357951</td>\n",
       "      <td>-1.338295</td>\n",
       "      <td>0.663417</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.316898</td>\n",
       "      <td>-0.249470</td>\n",
       "      <td>0.932273</td>\n",
       "      <td>-0.882030</td>\n",
       "      <td>-1.380060</td>\n",
       "      <td>-1.439971</td>\n",
       "      <td>0.568715</td>\n",
       "      <td>1.360911</td>\n",
       "      <td>-1.321978</td>\n",
       "      <td>0.681733</td>\n",
       "      <td>-1.230541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.326959</td>\n",
       "      <td>-0.077878</td>\n",
       "      <td>0.886235</td>\n",
       "      <td>-0.909656</td>\n",
       "      <td>-1.376705</td>\n",
       "      <td>-1.452072</td>\n",
       "      <td>0.575071</td>\n",
       "      <td>1.415351</td>\n",
       "      <td>-1.311100</td>\n",
       "      <td>0.734340</td>\n",
       "      <td>-1.230541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.315921</td>\n",
       "      <td>0.093714</td>\n",
       "      <td>0.802715</td>\n",
       "      <td>-0.902683</td>\n",
       "      <td>-1.386049</td>\n",
       "      <td>-1.433920</td>\n",
       "      <td>0.596681</td>\n",
       "      <td>1.346012</td>\n",
       "      <td>-1.321978</td>\n",
       "      <td>0.639836</td>\n",
       "      <td>-1.229909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0 -1.439778 -0.826644  1.281436 -0.921232 -1.379101 -1.488376  0.585240   \n",
       "1 -1.449601 -0.748647  1.304564 -0.921495 -1.363528 -1.482325  0.585240   \n",
       "2 -1.434721 -0.686250  1.219086 -0.944385 -1.351309 -1.476275  0.568715   \n",
       "3 -1.413702 -0.623853  1.169060 -0.946884 -1.348194 -1.464173  0.583969   \n",
       "4 -1.368693 -0.545857  1.161883 -0.924389 -1.354663 -1.458123  0.582698   \n",
       "5 -1.328821 -0.389864  0.984836 -0.890186 -1.374309 -1.452072  0.581427   \n",
       "6 -1.316898 -0.249470  0.932273 -0.882030 -1.380060 -1.439971  0.568715   \n",
       "7 -1.326959 -0.077878  0.886235 -0.909656 -1.376705 -1.452072  0.575071   \n",
       "8 -1.315921  0.093714  0.802715 -0.902683 -1.386049 -1.433920  0.596681   \n",
       "\n",
       "         7         8         9         10  \n",
       "0  1.387845 -1.357331  0.532012 -1.231172  \n",
       "1  1.393002 -1.363676  0.568733 -1.229909  \n",
       "2  1.363586 -1.360957  0.552938 -1.230541  \n",
       "3  1.382878 -1.356424  0.548933 -1.229909  \n",
       "4  1.348591 -1.350985  0.574179 -1.229909  \n",
       "5  1.357951 -1.338295  0.663417 -1.229909  \n",
       "6  1.360911 -1.321978  0.681733 -1.230541  \n",
       "7  1.415351 -1.311100  0.734340 -1.230541  \n",
       "8  1.346012 -1.321978  0.639836 -1.229909  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2912f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b83965d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       -1.231172\n",
       "1       -1.229909\n",
       "2       -1.230541\n",
       "3       -1.229909\n",
       "4       -1.229909\n",
       "           ...   \n",
       "15034   -1.426381\n",
       "15035   -1.415642\n",
       "15036   -1.516089\n",
       "15037   -1.481343\n",
       "15038   -1.428277\n",
       "Name: 10, Length: 15039, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3451c03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x1a740439160>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building the model structure\n",
    "model = Sequential()\n",
    "model.add(Dense(22, input_dim=10, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(12,kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c3198ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the model\n",
    "model.compile(loss=\"mse\",optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "676822d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1008/1008 [==============================] - 6s 3ms/step - loss: 0.5972 - accuracy: 0.0000e+00 - val_loss: 0.6256 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5279 - accuracy: 0.0000e+00 - val_loss: 0.6254 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5273 - accuracy: 0.0000e+00 - val_loss: 0.6244 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5269 - accuracy: 0.0000e+00 - val_loss: 0.6242 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5266 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5265 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5264 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5263 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5262 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5262 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5262 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5261 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5261 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5261 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5261 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6244 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5260 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5259 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6248 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6240 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6239 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5258 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a7414b59d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building the model for different epochs\n",
    "model.fit(x,y, validation_split=0.33,epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8233d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "394d6593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6243 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6241 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 53/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 54/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 57/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 59/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 78/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 84/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 88/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 101/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 108/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 113/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1008/1008 [==============================] - 3s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5257 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 123/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 136/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 137/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 138/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6238 - val_accuracy: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6231 - val_accuracy: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6236 - val_accuracy: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6237 - val_accuracy: 0.0000e+00\n",
      "Epoch 147/150\n",
      "1008/1008 [==============================] - 2s 2ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6234 - val_accuracy: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6232 - val_accuracy: 0.0000e+00\n",
      "Epoch 149/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6233 - val_accuracy: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1008/1008 [==============================] - 3s 3ms/step - loss: 0.5256 - accuracy: 0.0000e+00 - val_loss: 0.6235 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x,y, validation_split=0.33,epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "207479f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting for the train and test\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df3cc74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acd5b1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f066841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f10c08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
